#!/bin/bash
#
#SBATCH --job-name=pre-processing
#SBATCH --output=pre-processing.out
#
### Modify this according to your Ray workload.
#SBATCH --nodes=1
##SBATCH --exclusive
#
# Number of tasks needed for this job. Generally, used with MPI jobs
#SBATCH --ntasks=1
#SBATCH --partition=short
#
# Number of CPUs allocated to each task. 
#SBATCH --cpus-per-task=5
#
# Mimimum memory required per allocated  CPU  in  MegaBytes. 
#SBATCH --mem-per-cpu=10G
#
# Send mail to the email address when the job fails
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=tmamidi@uab.edu

#Set your environment here
module load Anaconda3/2020.02
#source activate testing
source activate training

#Run your commands here
#Run your commands here
#wget -P /data/project/worthey_lab/projects/experimental_pipelines/tarun/ditto/data/external/ https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/clinvar.vcf.gz
#gunzip /data/project/worthey_lab/projects/experimental_pipelines/tarun/ditto/data/external/clinvar.vcf.gz
####vcffilter -f "DP > 10 & MQ > 30 & QD > 20" SL156674.vcf > filtered_SL156674.vcf
#python /data/project/worthey_lab/projects/experimental_pipelines/tarun/ditto/annotation_parsing/parse_annotated_vars.py -i /data/project/worthey_lab/projects/experimental_pipelines/tarun/ditto/data/interim/merged_sig_norm_vep-annotated.vcf.gz -o /data/project/worthey_lab/projects/experimental_pipelines/tarun/ditto/data/interim/merged_sig_norm_vep-annotated.tsv
#until [ -f /data/project/worthey_lab/projects/experimental_pipelines/tarun/ditto/data/interim/merged_sig_norm_vep-annotated.tsv ]
#do
#    sleep 600
#done
#echo "parsing completed!"
#python /data/project/worthey_lab/projects/experimental_pipelines/tarun/ditto/src/training/data-prep/extract_class.py
python /data/project/worthey_lab/projects/experimental_pipelines/tarun/ditto/src/training/data-prep/filter.py
exit
#python final.py
